{
    "model_versions":{
        "v04":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"8, 16 filters of (3,3)",
            "dense"          :16,
            "batch_size"     :32,
            "reward_changing":"(length-initial_length)*food_reward",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :50000,
            "optimizer"      :"Adam(0.0001)"
        },
        "v05":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"8, 16 filters of (3,3)",
            "dense"          :16,
            "batch_size"     :64,
            "reward_changing":"(length-initial_length)*food_reward",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :50000,
            "optimizer"      :"Adam(0.0001)"
        },
        "v06":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"8, 16 filters of (3,3)",
            "dense"          :16,
            "batch_size"     :64,
            "reward_changing":"(length-initial_length)*food_reward",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :100000,
            "optimizer"      :"Adam(0.0001)"
        },
        "v07":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (3,3)",
            "dense"          :16,
            "batch_size"     :32,
            "reward_changing":"(length-initial_length)*food_reward",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :100000,
            "optimizer"      :"RMSprop(0.0005)"
        },
        "v08":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (3,3)",
            "dense"          :16,
            "batch_size"     :32,
            "reward_changing":"constant",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :100000,
            "optimizer"      :"RMSprop(0.0005)"
        },
        "v09":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :32,
            "reward_changing":"(length-initial_length)*food_reward",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :50000,
            "optimizer"      :"RMSprop(0.0005)"
        },
        "v10":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :32,
            "reward_changing":"(length-initial_length)*(food/death)",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :178000
        },
        "v10.1":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"constant",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :50000,
            "max_time_limit" :198,
        },
        "v10.2":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward, -0.1 time",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :60000,
            "max_time_limit" :298,
        },
        "v11":{
            "agent_type"     :"PolicyGradientAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :"epsiode length",
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :100000,
            "max_time_limit" :198,
            "others"         :"rewards normalized before input to training function\n
                               policy gradient mean over timesteps, entropy sum over timesteps"
        },
        "v12":{
            "agent_type"     :"PolicyGradientAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"4, 8 filters of (2,2)",
            "dense"          :16,
            "batch_size"     :"1",
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :6,
            "frames"         :2,
            "buffer_size"    :2000,
            "optimizer"      :"RMSprop(0.01)",
            "training_range" :10000,
            "entropy_coef"   :0.01,
            "max_time_limit" :198,
            "others"         :"rewards not normalized before input to training function\n
                               policy gradient mean over epsiodes, entropy sum over timesteps+epsiodes"
        },
        "v13":{
            "agent_type"     :"AdvantageActorCriticAgent",
            "reward"         :"-10/10 reward, -0.1 time",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :"1",
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :2000,
            "optimizer"      :"RMSprop(0.001) actor, RMSprop(0.0005) critic",
            "training_range" :10000,
            "entropy_coef"   :0.01,
            "max_time_limit" :198,
            "others"         :"rewards not normalized before input to training function\n
                               policy gradient mean over epsiodes, entropy mean over epsiodes"
        },
        "v14":{
            "agent_type"     :"HamiltonianCycleAgent",
            "board_size"     :10,
            "frames"         :1,
            "max_time_limit" :-1
        },
            "v15":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"32, 64 filters of (3,3)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"constant",
            "board_size"     :6,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :200000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber"
        },
            "v15.1":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"16, 32 filters of (3,3)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"constant",
            "board_size"     :10,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :200000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber"
        },            
            "v15.2":{
            "agent_type"     :"BreadthFirstSearchAgent+SupervisedAgent+DeepQLearningAgent",
            "reward"         :"-10/10 reward",
            "conv_filters"   :"16, 32 filters of (4,4)",
            "dense"          :64,
            "batch_size"     :32,
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :10,
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :50000,
            "max_time_limit" :"18 for BreadthFirstSearchAgent, 998 for DeepQLearningAgent"
        },
            "v15.3":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"16, 32 filters of (3,3)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"(length-initial_length)*(food_reward)",
            "board_size"     :10,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :200000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber"
        },
            "v15.4":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"16, 32 filters of (3,3)",
            "dense"          :64,
            "batch_size"     :128,
            "reward_changing":"constant",
            "board_size"     :10,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :100000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber"
        },
            "v16":{
            "agent_type"     :"BreadthFirstSearchAgent",
            "board_size"     :10,
            "frames"         :2,
            "max_time_limit" :-1
        },
            "v17":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"16, 32, 64 filters of (3,3), (3,3), (6,6)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"constant",
            "board_size"     :10,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :100000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber",
            "obstacles"      :"12 randomly generated connected boards"
        },            
        "v17.1":{
            "agent_type"     :"DeepQLearningAgent",
            "reward"         :"-1/1 reward",
            "conv_filters"   :"16, 32, 64 filters of (3,3), (3,3), (6,6)",
            "dense"          :64,
            "batch_size"     :64,
            "reward_changing":"constant",
            "board_size"     :10,
            "board_normalize":"divide by 4.0",
            "reward_clipping":"True",
            "frames"         :2,
            "buffer_size"    :60000,
            "optimizer"      :"RMSprop(0.0005)",
            "training_range" :300000,
            "max_time_limit" :998,
            "tf_seed"        :42,
            "environment"    :"SnakeNumpy, 16*8 frames per step played, 8 games evaluated",
            "Loss"           :"Huber",
            "obstacles"      :"40008 randomly generated connected boards, 15 cells obstacles"
        }
    }
}
